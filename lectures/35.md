# Lecture 35

## Priority queues

Data structures optimized for repeated removal of the smallest element.

Applications:

1. Air traffic control: airplanes hav events with timestamps.
Dispatchers repeately remove and service the earliest timestamps, 24 hours a day.
2. Assembling packets from a video stream: depending on protocol they may have arrived out of order.
We add them to a priority queue, then repeated remove the packet with the earliest frame.
3. Scientific and military simulations: multitudes of objects generating events and state changes, each with a timestamp.
The simulation processor will repeatedly dequeue min timestamp and service it.

Implementing priority with any linear structures is naive:

unsorted array:
* add is O(1)
* removeMin() is O(n)

sorted array:
* add is O(1)
* remove() is O(n)

linked List:
* add is O(1)
* remove() is O(n)

our hash table implementation:
* add is O(1)
* remove is O(n)

We use trees for better priority queues, and in particular heap is an efficient implementation of a priority queue.

## Heaps

3 properties:

1. Complete binary trees.
This means all children are present unless they are on the last row, irght-size where they can be present optionally.
2. Root is smaller than or equal to either child in no particular order.
3. They support duplicates

Because heaps are complete binary trees, we do not need to construct nodes and can represent a heap with just an array.
Children are linked to parents with arithmetic:

```
If parent index is i,

leftChildIndex is 2i+1
rightChildINdex is 2i+2
parentIndex = (childIndex - 1) / 2
```

root is always at index 0.

## Peek(), Add(), Remove()

Peek: return arr[0]; O(1)
Add(): add the last leaf at index size, then percolate up until the leaf is not smaller than it's parent.
Runtime complexitiy is O(Height)
Remove(): remove root at index 0; replace it with the last lead at index[size - 1]
Runtime complexity is O(Height)

BUT
we have a complete binary tree, which branches EVERYWHERE it could possibly branch.
It is the MOST SPLAYED tree that could exist with the given data items.

This guarantees the height is logarithmic, so Add() and Remove() and O(logn)

Real-world consideration: we can loado the heap array in cache and achieve thousands of operations per memory read.

### Building a heap out of an array of random elements:

One way to do it is to add() each element.

n elements* O(logn) insertion is O(nlogn) to build a heap.

HOWEVER there is an O(n) algorithm to build a heap.
It is alled Heapify():
"sort the arbitray array in place, except we're not sorting, just creating a heap structure."

1. Last parent is at size/2 -1. 
PercolateDown() each parent until and including root.

2.  Runtime complexity:
    Fraction of n ansectors * logn per percolate so O(nlogn) but this is not the tightest possible bound!
    O(n) is a tighter bound for heapify():

    We hand count the iterators of percolateDown() accross all nodes.

    ```
    parents   grandparents    great-grandparents
    n/4 * 1 + n/8 * 2 swaps + n/16 * 3 swaps
    ```

    The terms shrink in size because the number of nodes in each term is HALVED every new term, while the number of iterations per node incrases by 1:

    So the shrinking is exponential, which the growth is linear.

    The above series adds to ~2-3n.

    So heapify is O(n) and it is also in-place for an arbitrary array.

    **EXAM QUESTION** Write the code for heapify:

    ```
    heapify(data, current):
        current = size/2 - 1;
        while (current >= 0)
            percolateDown(data, current);
            --current;
    ```

### Heapsort

Given an arbitrary array we can heapify() in O(n) then we removeMin() into a new array until we sort everything.
Each element percolates in O(logn).
Overall time is O(nlogn).

We can heapify into a max-heap and that will let us sort in place!

```
int last = size - 1;
while (last > 0):
    A[last] = remove();
    last--;
```