# Lecture 35

## Hashing

The hash function converts some object into a very large integer in constant time (usuaully with hardware bit-shift operations for speed up)

The resulting VeryLargeInteger is remapped to the size of the hash table by modulo arithmetic.

index = VeryLargeInteger % arr.size, giving

Collisions occur when two or more objects map to the same index.

Two options for "enduring" collisions that do no mitigation:

1. Chaining: put a linked list at the hash index and have multiple objects to look through after the hash has been completed.

We call hash indices "buckets" when chaining is used because we have to "rifle though" buckets of information.

2. Linear probing: allow only one element per index and deposit colliding objects in the first available emtpy slot.allow only one element per index and deposit colliding objects in the first available emtpy slot.allow only one element per index and deposit colliding objects in the first available emtpy slot.allow only one element per index and deposit colliding objects in the first available emtpy slot.allow only one element per index and deposit colliding objects in the first available emtpy slot.allow only one element per index and deposit colliding objects in the first available emtpy slot.allow only one element per index and deposit colliding objects in the first available emtpy slot.allow only one element per index and deposit colliding objects in the first available emtpy slot.allow only one element per index and deposit colliding objects in the first available emtpy slot. 
Search for an itemslot by slot, wrapping around the end of the array and ending at original_index - 1 or first empty slot.
Deleted keys can terminate searches early: for example, in the powerpoint slides, if we delete 14 and then search for 35 we terminate the search early without finding it when we shouldh ave.
We fix that issue with tombstone hashing, where deleted items are subsituted by special values, e.g. -1, to singal to the search loop to not terminate.
For chaining, having a few collisions at any node is robush: the search still takes O(1) because it includes hashing + checking.
But if a signigicant fraction of input is assigned at the same index, search performance may degrade to linear.

Big-o performance is not the whole picture: in the real world linear probing is signigicantly faster than chaining BECAUSE of cache locality:
We can load a whole cache table in the processor cache and check thousands of indices for the cost of a single memory access.

Mitigation techniques to avoid collisions:

1. Rehashing: changing the table size (in indices) when the table becomes "saturated".
Let n be the number of items inserted.
Let M be the number of indices.
Let alpha, the load factor, be defined as alpha = n/M.
The hash table implementation monitors this ratio and incresaes in size if there are too many keys relative to indices.
This is equivalent to saying "I will provide you a surfeit of bucketss so it is less likeley for your hash function to result in colisions.
Java initializes HashMap at size 16; and it doubles the hash tables when alpha, the load factor is 0.75.
Which means inserting a 12th element will double the backing array size, resulting in a load factor of 12/32.
after the size has been chained, all entries need to be rehashed in order to distribute them uniformly among the indices.
The extra rehashing is at a cost amortized O(1) per element: inserting n elements accures O(n) extra rehashing operations.
When the load factor drops to 25%, the table shrinks by half.
The intermittent rehashing means we cannot maintain the original order of insertion of elements.
If we need to preserve the original order, we can use LinkedHashMap, which guarantees the order of iteration.

2. Choose a good hashing function
    * Examples of bad hashing functions
        1. Sum up the ascii values of the characters
            * Promotes collisions of anagrams
            * Does not use all information of each key: sequence information disgarded
        2. Take the first 3 characters of the string and have them have their own hash/bucket number.
            * In English, some three letter combinations are extremely common
            * Conversely, index "xzq" is never going to be used.
            * We still ommited a lot of information when creating the hash.
    * Properties of a good hash function
        1. It needs to be deterministic, so you cannot use rand()
        2. It needs to have a uniform distribution
        3. Needs to be fast to compute. O(1) time
        4. Needs to utilize all information present in original key.
    * Example of a good hash function
         ```java
        public int hashCode() {
            int hash = 0;
            for (int i = 0; i < length(); i++) {
                hash = hash * 31 + charAt(i);
            }
            return hash;
        }
        ```
        We use prime numbers extensively in hash functions because prime numbers almost never form patterns compared to normal numbers.
        31 and 127 are Mersenne primes: prime numbers that have the form 2^(k-1). 31=2^5-1 127 = 2^7-1
        These are hardware-friendly, allowing quick computations with bit shift operations.

Wrap-up of hashing:
* Determining which instance variables are used by equals() Hashing each of them, and combining them with some sequence-respecting accumulator expression such as:

```java
c = myInstanceVariable.hashCode()
hash = hash * 31 + c;
```

If some instance variables are collections of many objects, such a collection of LinkedIn contacts then hashing may take O(n) which is slow.

Takeaway: Maybe we shouldn't be using massive objects as hash table keys.